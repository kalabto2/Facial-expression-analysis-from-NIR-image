{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5f947b0",
   "metadata": {},
   "source": [
    "# Inference \n",
    "\n",
    "---\n",
    "\n",
    "The inference is created from 3 parts - face detector, spectrum translator from NIR to VIS and vice versa, and Facial expression recognition part.\n",
    "\n",
    "### Face Detector\n",
    "Now, face detection can be done with 2 following models:\n",
    "* **RetinaFace** - model from [here](https://github.com/serengil/deepface). Accurate, though less quick. Also, RetinaFace can align the face.\n",
    "* **CenterFace** - original Centerface from [here](https://gitlab.fit.cvut.cz/vadlemar/real-time-facial-expression-recognition-in-the-wild). It is fast, however less accurate than first one. The model in onnx is stored in `models/face_detection/centerface.onnx`. This option does not implicitly align faces in Inference class, RetinaFace does.\n",
    "\n",
    "Additional detectors from DeepFace module can be easily added such as *MTCNN* or *Jones-Viola* algorithm.\n",
    "\n",
    "### Spectrum Translator\n",
    "Translates images between NIR, VIS specters.\n",
    "\n",
    "* **CycleGAN on CASIA+OuluCasia db** - This Translates between both specters. \n",
    "* **FFE-CycleGAN on BUAA db** - This translates between NIR images and averaged grayscale image (averaged from color green illuminated RGB image). The translation is good, however not \n",
    "\n",
    "### Facial expression recognition\n",
    "Classifies categorical emotion and regresses the valece arousal labels. \n",
    "* **MobileNet** - FER net from original work [here](https://github.com/serengil/deepface). Does not work on NIR. Onnx in `models/pretrained/mobilenet_simultaneous.onnx`.\n",
    "* **MobileNet-NIR** - Original MobileNet pretrained on Oulu-Casia database NIR images. Several onnx versions are stored in a folder `models/mobilenet_NIR/`. Additional information below.\n",
    "\n",
    "***\n",
    "\n",
    "#### MobileNet-NIR on Oulu-Casia db\n",
    "This was pretrained on original mobilenet facial expression recognition model.\n",
    "It has both categorical and spatial predictions.\n",
    "\n",
    "It was pretrained on 7 emotions (neutral, anger, disgust, fear, happy, sad, surprise) - without contempt unlike in original model.\n",
    "\n",
    "\n",
    "Since this dataset the model was pretrained on did not have valence arousal labels (only categorical), they were assigned in following way. Because dataset contained for each emotion (for each patient) images retrieved from video, that started from neutral to most affected emotion. according to this gradient were linearly assigned valence arousal values.\n",
    "\n",
    "The most affected categorical emotions are anchored to following V/A values:\n",
    "\n",
    "* neutral: (0., 0.),\n",
    "* anger: (-0.51, 0.59),\n",
    "* disgust: (-0.60, 0.35),\n",
    "* fear: (-0.64, 0.6),\n",
    "* happiness: (0.81, 0.51),\n",
    "* sadness: (-0.63, -0.27),\n",
    "* surprise: (0.4, 0.67)\n",
    "\n",
    "Above VA labels are from [-1, 1] and are not recalculated from circumplex model yet.\n",
    "But model can be quickly trained, and now is only pretrained on 15 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec27ae3",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba66b272",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c369680",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T12:38:32.579146Z",
     "start_time": "2024-03-14T12:38:28.503428Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-14 13:38:29.035588: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-14 13:38:29.089118: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-14 13:38:29.089776: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-14 13:38:29.960391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from skeleton.inference import Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4083a25e",
   "metadata": {},
   "source": [
    "Definitions of inference classes of individual networks and one encapsulating inference class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85866fe",
   "metadata": {},
   "source": [
    "## Inference examples\n",
    "\n",
    "***\n",
    "\n",
    "The inference first needs defining each of: *face detector*, *spectrum translator* and *facial expression recognition* (FER). Each one of them needs either `\"net_type\": None` for skipping this part, or defining the model type with its path to onnx file and additional info.\n",
    "This models definition are shown below and further passed to Inference object.\n",
    "\n",
    "Model types can be for:\n",
    "* **Face detector** - `Inference.net_type.FACE_DETECTOR_CENTERFACE` or `Inference.net_type.FACE_DETECTOR_RETINAFACE`\n",
    "* **FER** - `Inference.net_type.FER_MOBILENET`\n",
    "* **Spectrum translation** - `Inference.net_type.SPECTRUM_TRANSLATOR_FFE_CYCLEGAN` or\n",
    "`Inference.net_type.SPECTRUM_TRANSLATOR_ORIG_CYCLEGAN`\n",
    "\n",
    "Each model is in the folder *models/*.\n",
    "\n",
    "***\n",
    "\n",
    "After the `Inference` object definition, the inference can be run. Either *from_folder* loading images, *from_filenames* loading images from array of filepaths, or *from_array* - already loaded image as np.array.\n",
    "\n",
    "Also the inference can be run as *infer_instant* where each image is immediately passed through all parts of Inference.\n",
    "<!-- Inference, as opposed to *infer* mode, where are first all images detected, then than those translated and then FER. -->\n",
    "\n",
    "<!-- Thus following functions are available:\n",
    "* `inf.infer_from_folder('path/to/folder')`\n",
    "* `inf.infer_from_filenames(['path/to/image1', 'path/to/image2'])`\n",
    "* `inf.infer_from_array(img_np_array)` -->\n",
    "\n",
    "<!-- And for Inference of each image separately is: -->\n",
    "* `inf.infer_instant_from_folder('path/to/folder', save_to_folder='pth/to/save_fld')`\n",
    "* `inf.infer_instant_from_filenames(['path/to/image1', 'path/to/image2'], save_to_folder='pth/to/save_fld')`\n",
    "* `inf.infer_instant_from_array(img_np_array, save_to_folder='pth/to/save_fld')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "842e215c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T13:40:33.910572Z",
     "start_time": "2024-03-14T13:40:29.681746Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-14 14:40:30.189214: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-14 14:40:30.244310: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-14 14:40:30.245451: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-14 14:40:31.131221: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using '{'net_type': <net_type.FACE_DETECTOR_RETINAFACE: 'G'>, 'retina_face_align': True, 'remove_black_stripes': False, 'display_images': False, 'save_image_to_folder': None}' as face detector model\n",
      "Using '{'net_type': <net_type.SPECTRUM_TRANSLATOR_ORIG_CYCLEGAN: 'B'>, 'pth_to_onnx': 'models/spectrum_translation/VIS2NIR_cyclegan_snellius_casia_oulucasia_double_gen_opt-GA-latestepoch.onnx', 'input_as_avg_grayscale': False, 'output_as_avg_grayscale': True, 'display_images': True, 'save_image_to_folder': 'transtmp'}' as spectrum transfer model\n",
      "Orig-CycleGAN model from 'models/spectrum_translation/VIS2NIR_cyclegan_snellius_casia_oulucasia_double_gen_opt-GA-latestepoch.onnx' loaded.\n"
     ]
    }
   ],
   "source": [
    "from skeleton.inference import Inference\n",
    "\n",
    "# define here which models to use. If u which not to use certain model, set its 'net_type' = None.\n",
    "# If you want to apply model, pick from Inference.net_type defined above\n",
    "models = {\n",
    "    \"face_detector\": {\n",
    "        \"net_type\": Inference.net_type.FACE_DETECTOR_RETINAFACE,\n",
    "        # If using RetinaFace detector, when true, align faces.\n",
    "        # Should not be used alongside 'remove_black_stripes' == True (artifacts wil be created)\n",
    "        \"retina_face_align\": True,\n",
    "        # Returned images have square dimenstions, however, detectors return rectangles,\n",
    "        # thus there are black stripes on sides. If you wish to remove those stripes, set True.\n",
    "        \"remove_black_stripes\": False,\n",
    "        # Display detected images in notebook\n",
    "        \"display_images\": False,\n",
    "        # Save images - when loded from filepath saved as <original_filestem>_<face_idx>.<original_extension>,\n",
    "        # else as <detection_id>.jpg starting from 0\n",
    "        \"save_image_to_folder\": None,\n",
    "    },\n",
    "    \"spectrum_translator\": {\n",
    "        \"net_type\": Inference.net_type.SPECTRUM_TRANSLATOR_ORIG_CYCLEGAN,\n",
    "        \"pth_to_onnx\": 'models/spectrum_translation/VIS2NIR_cyclegan_snellius_casia_oulucasia_double_gen_opt-GA-latestepoch.onnx',\n",
    "        # First translates input image to grayscale and then translates spectra\n",
    "        \"input_as_avg_grayscale\": False,\n",
    "        # Translates newly translated image to averaged grayscale\n",
    "        \"output_as_avg_grayscale\": True,\n",
    "        \"display_images\": True,\n",
    "        # Saves as <global idx_xount>_<id of face starting from 0>.jpg\n",
    "        \"save_image_to_folder\": \"transtmp\",\n",
    "    },\n",
    "    \"fer\": {\n",
    "        \"net_type\": None, # Inference.net_type.FER_MOBILENET,\n",
    "        \"pth_to_onnx\": 'models/_old/mobilenet_NIR/mobilenet_on_AffectNet-NIR/mobilenet_aff_nir-aff_continue.onnx',\n",
    "        \"display_images\": True,\n",
    "        # Saves as <global idx_xount>_<id of face starting from 0>.jpg\n",
    "        \"save_image_to_folder\": \"fertmp\",\n",
    "    }\n",
    "}\n",
    "# debug flag is displaying images, verbose prints other secondary information\n",
    "inf = Inference(models, None, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe0b07a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-14T13:40:30.217Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "output = inf.infer_instant_from_folder('../DIP/Facial-expression-analysis-from-NIR-image/data/AffectNet-8Labels/train_set/images_small/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb7fad4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvDIP",
   "language": "python",
   "name": "venvdip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

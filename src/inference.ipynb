{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5f947b0",
   "metadata": {},
   "source": [
    "# Inference \n",
    "<!-- \n",
    "---\n",
    "\n",
    "The inference is created from 3 parts - face detector, spectrum translator from NIR to VIS and vice versa, and Facial expression recognition part.\n",
    "\n",
    "### Face Detector\n",
    "Now, face detection can be done with 2 following models:\n",
    "* **RetinaFace** - model from [here](https://github.com/serengil/deepface). Accurate, though less quick. Also, RetinaFace can align the face.\n",
    "* **CenterFace** - original Centerface from [here](https://gitlab.fit.cvut.cz/vadlemar/real-time-facial-expression-recognition-in-the-wild). It is fast, however less accurate than first one. The model in onnx is stored in `models/face_detection/centerface.onnx`. This option does not implicitly align faces in Inference class, RetinaFace does.\n",
    "\n",
    "Additional detectors from DeepFace module can be easily added such as *MTCNN* or *Jones-Viola* algorithm.\n",
    "\n",
    "### Spectrum Translator\n",
    "Translates images between NIR, VIS specters.\n",
    "\n",
    "* **CycleGAN on CASIA+OuluCasia db** - This Translates between both specters. \n",
    "* **FFE-CycleGAN on BUAA db** - This translates between NIR images and averaged grayscale image (averaged from color green illuminated RGB image). The translation is good, however not \n",
    "\n",
    "### Facial expression recognition\n",
    "Classifies categorical emotion and regresses the valece arousal labels. \n",
    "* **MobileNet** - FER net from original work [here](https://github.com/serengil/deepface). Does not work on NIR. Onnx in `models/pretrained/mobilenet_simultaneous.onnx`.\n",
    "* **MobileNet-NIR** - Original MobileNet pretrained on Oulu-Casia database NIR images. Several onnx versions are stored in a folder `models/mobilenet_NIR/`. Additional information below. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec27ae3",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba66b272",
   "metadata": {},
   "source": [
    "Imports - this needs to be imported from the root directory - in that directory needs to be folder `skeleton`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c369680",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T12:38:32.579146Z",
     "start_time": "2024-03-14T12:38:28.503428Z"
    }
   },
   "outputs": [],
   "source": [
    "from skeleton.FaceInference.inference import Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35687b46-0e76-4e66-8050-ed772cdfea97",
   "metadata": {},
   "source": [
    "Another way how to import the package is just importing like regular package like that:\n",
    "\n",
    "```\n",
    "from FaceInference.inference import Inference\n",
    "```\n",
    "\n",
    "and it should be callable from everywhere in the scope, it is installed in.\n",
    "However, this assumes that that package has been installed locally:\n",
    "\n",
    "```\n",
    "cd skeleton # switch directory from root of the projet to the package folder\n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "Also, when run this notebook with uage of this package, the correct kernel needs to be set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c580a2b-9728-4ef3-b537-150918fef006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from FaceInference.inference import Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85866fe",
   "metadata": {},
   "source": [
    "## Inference examples\n",
    "\n",
    "***\n",
    "\n",
    "The inference first needs defining each of: *face detector*, *spectrum translator* and *facial expression recognition* (FER). Each one of them needs either `\"net_type\": None` for skipping this part, or defining the model type with its path to onnx file and additional info.\n",
    "This models definition are shown below and further passed to Inference object.\n",
    "\n",
    "Model types can be for:\n",
    "* **Face detector** - `Inference.net_type.FACE_DETECTOR_CENTERFACE` or `Inference.net_type.FACE_DETECTOR_RETINAFACE`\n",
    "* **FER** - `Inference.net_type.FER_MOBILENET`\n",
    "* **Spectrum translation** - `Inference.net_type.SPECTRUM_TRANSLATOR_ORIG_CYCLEGAN`\n",
    "\n",
    "Each model is stored in the folder *models/*.\n",
    "\n",
    "***\n",
    "\n",
    "After the `Inference` object definition, the inference can be run. Either *from_folder* loading images, *from_filenames* loading images from array of filepaths, or *from_array* - already loaded image as np.array.\n",
    "Additionally, the *infer_video* provides incorporating the FER results right into the original video.\n",
    "\n",
    "<!-- Also the inference can be run as *infer_instant* where each image is immediately passed through all parts of Inference. -->\n",
    "<!-- Inference, as opposed to *infer* mode, where are first all images detected, then than those translated and then FER. -->\n",
    "\n",
    "<!-- Thus following functions are available:\n",
    "* `inf.infer_from_folder('path/to/folder')`\n",
    "* `inf.infer_from_filenames(['path/to/image1', 'path/to/image2'])`\n",
    "* `inf.infer_from_array(img_np_array)` -->\n",
    "\n",
    "<!-- And for Inference of each image separately is: -->\n",
    "* `inf.infer_instant_from_folder('path/to/folder', save_to_folder='pth/to/save_fld')`\n",
    "* `inf.infer_instant_from_filenames(['path/to/image1', 'path/to/image2'], save_to_folder='pth/to/save_fld')`\n",
    "* `inf.infer_instant_from_array(img_np_array, save_to_folder='pth/to/save_fld')`\n",
    "* `inf.infer_video('path/to/input_video.mp4', 'path/where/to/save/output_video.mp4', fraes_per_second)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e10ff59",
   "metadata": {},
   "source": [
    "### VIS->NIR Spectrum translation example and FER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938be79a",
   "metadata": {},
   "source": [
    "This is example of translating images to NIR using the model from Experiment1.2. When Experiment1.1 wmat to be used, simply change path to `models/spectrum_translation/experiment1_1_VIS2NIR.onnx`. Subsequently, image is applied to the FER model from the *Experiment2.2*. Images can be ofcourse evlauated by FER model without using spextrum translation, as that is the discussed *Approach 2* in the thesis. However here it is used for demostrative purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842e215c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T17:11:57.851597Z",
     "start_time": "2024-05-08T17:11:53.025011Z"
    }
   },
   "outputs": [],
   "source": [
    "from skeleton.FaceInference.inference import Inference\n",
    "\n",
    "# define here which models to use. If u which not to use certain model, set its 'net_type' = None.\n",
    "# If you want to apply model, pick from Inference.net_type defined above\n",
    "models = {\n",
    "    \"face_detector\": {\n",
    "        \"net_type\": Inference.net_type.FACE_DETECTOR_CENTERFACE,\n",
    "        # If using RetinaFace detector, when true, align faces.\n",
    "        # Should not be used alongside 'remove_black_stripes' == True (artifacts wil be created)\n",
    "        \"retina_face_align\": True,\n",
    "        # Returned images have square dimenstions, however, detectors return rectangles,\n",
    "        # thus there are black stripes on sides. If you wish to remove those stripes, set True.\n",
    "        \"remove_black_stripes\": False,\n",
    "        # Display detected images in notebook\n",
    "        \"display_images\": False,\n",
    "        # Save images - when loded from filepath saved as <original_filestem>_<face_idx>.<original_extension>,\n",
    "        # else as <detection_id>.jpg starting from 0\n",
    "        \"save_image_to_folder\": None,\n",
    "    },\n",
    "    \"spectrum_translator\": {\n",
    "        \"net_type\": Inference.net_type.SPECTRUM_TRANSLATOR_ORIG_CYCLEGAN,\n",
    "        \"pth_to_onnx\": 'models/spectrum_translation/experiment1_2_VIS2NIR.onnx',\n",
    "        # First translates input image to grayscale and then translates spectra\n",
    "        \"input_as_avg_grayscale\": False,\n",
    "        # Translates newly translated image to averaged grayscale\n",
    "        \"output_as_avg_grayscale\": False,\n",
    "        # simply displays images\n",
    "        \"display_images\": True,\n",
    "        # Saves as <global idx_xount>_<id of face starting from 0>.jpg\n",
    "        \"save_image_to_folder\": None,\n",
    "    },\n",
    "    \"fer\": {\n",
    "        \"net_type\": Inference.net_type.FER_MOBILENET,\n",
    "        \"pth_to_onnx\": 'models/fer/experiment2.2-mobilent-affnir.onnx',\n",
    "        \"display_images\": True,\n",
    "        # Saves as <global idx_xount>_<id of face starting from 0>.jpg\n",
    "        \"save_image_to_folder\": None,\n",
    "    }\n",
    "}\n",
    "# debug flag is displaying images, verbose prints other secondary information such as time of inference etc.\n",
    "inf = Inference(models, None, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d66b7fe",
   "metadata": {},
   "source": [
    "Applied to VIS images from AffectNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe0b07a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T16:56:03.395465Z",
     "start_time": "2024-05-08T16:55:17.094444Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output = inf.infer_instant_from_folder('example_data/aff_vis/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddb8025",
   "metadata": {},
   "source": [
    "### NIR->VIS Spectrum translation and FER (Approach 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90cdffb",
   "metadata": {},
   "source": [
    "This is example of translating images to VIS using the model from Experiment1.2. When Experiment1.1 want to be used, simply change path to `models/spectrum_translation/experiment1_1_NIR2VIS.onnx`.\n",
    "Subsequently, image is applied to the FER model from the the original work trained on VIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c512dd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T23:49:20.114079Z",
     "start_time": "2024-05-06T23:49:19.869798Z"
    }
   },
   "outputs": [],
   "source": [
    "from skeleton.FaceInference.inference import Inference\n",
    "\n",
    "# define here which models to use. If u which not to use certain model, set its 'net_type' = None.\n",
    "# If you want to apply model, pick from Inference.net_type defined above\n",
    "models = {\n",
    "    \"face_detector\": {\n",
    "        \"net_type\": Inference.net_type.FACE_DETECTOR_RETINAFACE,\n",
    "        # If using RetinaFace detector, when true, align faces.\n",
    "        # Should not be used alongside 'remove_black_stripes' == True (artifacts wil be created)\n",
    "        \"retina_face_align\": True,\n",
    "        # Returned images have square dimenstions, however, detectors return rectangles,\n",
    "        # thus there are black stripes on sides. If you wish to remove those stripes, set True.\n",
    "        \"remove_black_stripes\": False,\n",
    "        # Display detected images in notebook\n",
    "        \"display_images\": False,\n",
    "        # Save images - when loded from filepath saved as <original_filestem>_<face_idx>.<original_extension>,\n",
    "        # else as <detection_id>.jpg starting from 0\n",
    "        \"save_image_to_folder\": None,\n",
    "    },\n",
    "    \"spectrum_translator\": {\n",
    "        \"net_type\": Inference.net_type.SPECTRUM_TRANSLATOR_ORIG_CYCLEGAN,\n",
    "        \"pth_to_onnx\": 'models/spectrum_translation/experiment1_2_NIR2VIS.onnx',\n",
    "        # First translates input image to grayscale and then translates spectra\n",
    "        \"input_as_avg_grayscale\": False,\n",
    "        # Translates newly translated image to averaged grayscale\n",
    "        \"output_as_avg_grayscale\": False,\n",
    "        # simply displays images\n",
    "        \"display_images\": True,\n",
    "        # Saves as <global idx_xount>_<id of face starting from 0>.jpg\n",
    "        \"save_image_to_folder\": None,\n",
    "    },\n",
    "    \"fer\": {\n",
    "        \"net_type\": Inference.net_type.FER_MOBILENET,\n",
    "        \"pth_to_onnx\": 'models/fer/mobilenet_on_aff.onnx',\n",
    "        \"display_images\": True,\n",
    "        # Saves as <global idx_xount>_<id of face starting from 0>.jpg\n",
    "        \"save_image_to_folder\": None,\n",
    "    }\n",
    "}\n",
    "# debug flag is displaying images, verbose prints other secondary information such as time of inference etc.\n",
    "inf = Inference(models, None, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0273f3",
   "metadata": {},
   "source": [
    "Infernece on images from AffectNetNIR - images from AffetNet translated to NIR.\n",
    "Those images were not in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9c94f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T23:51:18.006913Z",
     "start_time": "2024-05-06T23:49:24.740482Z"
    }
   },
   "outputs": [],
   "source": [
    "output = inf.infer_instant_from_folder('example_data/aff_nir/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f266d82d",
   "metadata": {},
   "source": [
    "Those are mixture of training and testing data with the stripes. Experiment1.1 performs better on those data with stripes rather than data without stripes. On the other hand, Experiment1.2 performs better on data without stripes compared to this wit hstripes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670588d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T22:22:22.049748Z",
     "start_time": "2024-05-06T22:21:03.522099Z"
    }
   },
   "outputs": [],
   "source": [
    "output = inf.infer_instant_from_folder('example_data/train_nir/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395ca69a",
   "metadata": {},
   "source": [
    "Images mostly without stripes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e04e1cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T22:24:09.620914Z",
     "start_time": "2024-05-06T22:22:44.385731Z"
    }
   },
   "outputs": [],
   "source": [
    "output = inf.infer_instant_from_folder('example_data/train_nir_wo_stripes/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e3e774",
   "metadata": {},
   "source": [
    "### Video example - FER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3119f560",
   "metadata": {},
   "source": [
    "The inference loads the video and applies on it the models specified. This video feature displays FER results integrated right into the original video (models definitions must contain FER and face detections, else it does nothing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c339a65e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T23:37:41.248120Z",
     "start_time": "2024-05-06T23:37:14.798675Z"
    }
   },
   "outputs": [],
   "source": [
    "from skeleton.FaceInference.inference import Inference\n",
    "\n",
    "# define here which models to use. If u which not to use certain model, set its 'net_type' = None.\n",
    "# If you want to apply model, pick from Inference.net_type defined above\n",
    "models = {\n",
    "    \"face_detector\": {\n",
    "        \"net_type\": Inference.net_type.FACE_DETECTOR_CENTERFACE,\n",
    "        # If using RetinaFace detector, when true, align faces.\n",
    "        # Should not be used alongside 'remove_black_stripes' == True (artifacts wil be created)\n",
    "        \"retina_face_align\": True,\n",
    "        # Returned images have square dimenstions, however, detectors return rectangles,\n",
    "        # thus there are black stripes on sides. If you wish to remove those stripes, set True.\n",
    "        \"remove_black_stripes\": False,\n",
    "        # Display detected images in notebook\n",
    "        \"display_images\": False,\n",
    "        # Save images - when loded from filepath saved as <original_filestem>_<face_idx>.<original_extension>,\n",
    "        # else as <detection_id>.jpg starting from 0\n",
    "        \"save_image_to_folder\": None,\n",
    "    },\n",
    "    \"spectrum_translator\": {\n",
    "        \"net_type\": None, #Inference.net_type.SPECTRUM_TRANSLATOR_ORIG_CYCLEGAN,\n",
    "        \"pth_to_onnx\": 'path/to/model.onnx',\n",
    "        # First translates input image to grayscale and then translates spectra\n",
    "        \"input_as_avg_grayscale\": False,\n",
    "        # Translates newly translated image to averaged grayscale\n",
    "        \"output_as_avg_grayscale\": False,\n",
    "        \"display_images\": True,\n",
    "        # Saves as <global idx_xount>_<id of face starting from 0>.jpg\n",
    "        \"save_image_to_folder\": None,\n",
    "    },\n",
    "    \"fer\": {\n",
    "        \"net_type\": Inference.net_type.FER_MOBILENET,\n",
    "        \"pth_to_onnx\": 'models/fer/experiment2.2-mobilent-affnir.onnx',\n",
    "        \"display_images\": False,\n",
    "        # Saves as <global idx_xount>_<id of face starting from 0>.jpg\n",
    "        \"save_image_to_folder\": None,\n",
    "    }\n",
    "}\n",
    "# debug flag is displaying images, verbose prints other secondary information\n",
    "inf = Inference(models, None, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d10eaf1",
   "metadata": {},
   "source": [
    "Function belwo calls the processingof the video. Method accepts input video path, output video path where it will be stored and the frequency of frames per second. Example input video is intentionally \"doubled\" for demonstration that inference can handle multiple people in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85949d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T23:38:23.014937Z",
     "start_time": "2024-05-06T23:37:41.250229Z"
    }
   },
   "outputs": [],
   "source": [
    "inf.infer_video(\"example_data/multiemotions_example_doubled.mp4\", \"example_data/output2.mp4\", 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7eaadd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvDIP",
   "language": "python",
   "name": "venvdip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
